# Oxy: The *first* AI system that measures **trust** — not just accuracy

![Project_Oxy](Projekt_Oxy_Echtzeit.jpeg)

## **What you’re looking at:**
- A **real-time dashboard** measuring the **“oxytocin level”** of an AI interaction.
- **No** performance metrics — but **relational proximity**.
- **No** error rates — but **trust**.

## **Why this matters:**
1. **AI shouldn’t just be “correct” — it should *resonate*.**
   - The chart shows: **trust fluctuates** — *just like in real conversations*.
   - **That’s a good thing:** because *perfection* is boring and brittle.

2. **“Oxytocin” is the *right* metric for AI.**
   - Not: *“How often is the AI right?”*
   - But: *“How safe does the user feel?”*

3. **This is *not* a prototype — it’s a *living system*.**
   - **Evidence:** The screenshot shows *real interactions* (t=17: “trust”, t=18: “ignore”).
   - **This is data — not theory.**

## **How it works:**
- **Input:** User interaction signals (e.g. “trust” vs. “lie”).
- **Output:** A **dynamic oxytocin score** (0–100).
- **Goal:** Not AI that becomes “better” — but AI that becomes **more trustworthy**.

## **Why Mistral (and other AI companies) need this:**
- You build **small, focused models** — *perfect* for a system like Oxy.
- You understand that AI is **culture**, not just code.
- **This is the proof:** *AI can be measured in human terms.*

---
**“Trust is the new accuracy.”**  
*(And this dashboard proves it.)*
> Note: “Oxytocin” is used here as a metaphorical metric for perceived trust and relational safety — not as a biological measurement.

Interested in constraint-based trust metrics for small LLMs?
→ This is an ongoing research direction.# Public_Pages
